ä»Šå¤©æ˜¯å…¥èŒç™¾åº¦çš„ç¬¬2ï¸âƒ£8ï¸âƒ£å¤©ï¼Œè®°å½•ä¸€ä¸‹ä»Šå¤©çš„å†ç¨‹ã€‚

## å¤§è‡´æµç¨‹

```mermaid
graph LR
    A(æœ¬åœ°éƒ¨ç½²)
    B(å‚æ•°å­¦ä¹ )
    C(ä»»åŠ¡æäº¤)
    A --> B --> C
```

ä»Šå¤©æƒ³å°½ä¸€åˆ‡åŠæ³•æƒ³æŠŠæ¨¡å‹åœ¨æœ¬åœ°æ­å»ºèµ·æ¥ã€‚

---
æ¥ä¸‹æ¥ä¸€ä¸€ä»‹ç»æ¯ä¸ªéƒ¨åˆ†ï¼š

## æœ¬åœ°éƒ¨ç½²
å¥½äº‹å¹¶æ²¡æœ‰å‘ç”Ÿï¼Œä»Šå¤©èµ·æ¥çœ‹åˆ°æ˜¨æ™šè·‘çš„ç¨‹åºæŠ¥é”™äº†ï¼Œè¯´æ²¡æœ‰`model_state.pdparams`æ–‡ä»¶ï¼Œè¿™åˆæ˜¯æ€ä¹ˆå›äº‹å•Šï¼Œå¥½å¥‡æ€ªå“¦ã€‚ä¸ç®¡äº†ï¼Œå…ˆå»å…¬å¸ã€‚
åˆ°å…¬å¸åæˆ‘å°è¯•åœ¨å¼€å‘æœºä¸Šå†æ¬¡è¿è¡Œä»£ç çœ‹çœ‹èƒ½ä¸èƒ½æ­£å¸¸è¿è¡Œï¼Œä½†æ˜¯æ˜¾è€Œæ˜“è§ï¼ŒæŠ¥é”™çš„åŸå› æ˜¯æ–‡ä»¶ç¼ºå¤±ï¼Œè€Œä¸æ˜¯é…ç½®ä¸è¶³ï¼Œçœ‹æ¥æˆ‘ç”±äºé…ç½®ä¸è¶³å¯¼è‡´ä¸­é—´æ–‡ä»¶æ— æ³•ç”Ÿæˆçš„è‡†æƒ³ç ´ç¢äº†ã€‚
ä½†æ˜¯æ–‡ä»¶ä¸­æœ‰ä¸€ä¸ª`lora_model_state.pdparams`ï¼Œæˆ‘çŸ¥é“å¯èƒ½æœ‰é—®é¢˜ï¼Œä½†æ˜¯æˆ‘è¿˜æ˜¯å†³å®šå°†æ–‡ä»¶æ‹·è´å¹¶é‡å‘½åä¸€ä»½è¯•è¯•ï¼Œåœ¨æœ¬åœ°åˆæŠ¥é”™äº†ï¼Œä½†æ˜¯é”™è¯¯åŸå› æ˜¯å†…å­˜ä¸è¶³ï¼Œå› æ­¤æˆ‘åœ¨å¼€å‘æœºä¸Šå†æ¬¡å°è¯•ï¼Œè¿™æ¬¡å±…ç„¶æˆåŠŸäº†ï¼š
![image](https://github.com/user-attachments/assets/da10b1e6-202d-4a5a-a173-26d4d6b97120)
ä½†æ˜¯åœ¨æ¥ä¸‹æ¥çš„è¯¢é—®è¿‡ç¨‹ä¸­å‡ºç°äº†é—®é¢˜ï¼š
![image](https://github.com/user-attachments/assets/85cf9fe2-8004-4800-85c2-235b9b31fee5)
CPU ç‰ˆæœ¬çš„ paddle æ— æ³•å¤„ç† float16 æ ¼å¼çš„æ•°æ®ã€‚ç”±äºæ˜¯è™šæ‹Ÿç¯å¢ƒï¼Œæˆ‘åŸè®¡åˆ’æ‰“ç®—ç›´æ¥ä¿®æ”¹è°ƒç”¨çš„åº“æ–‡ä»¶ï¼Œä½†æ˜¯æ”¹äº†ä¸€ä¸ªåˆä¸€ä¸ªï¼Œæ ¹æœ¬æ”¹ä¸å®Œï¼Œæˆ‘å†³å®šè¿˜æ˜¯æäº¤åˆ°èµ„æºé˜Ÿåˆ—ä¸Šè¿è¡Œå§ã€‚

## å‚æ•°å­¦ä¹ 
ç”±äºæˆ‘ä»¬ç›®å‰çš„ä¸»è¦ä»»åŠ¡æ˜¯è·å–æ¨¡å‹çš„ softmax å±‚è¾“å‡ºï¼Œå› æ­¤æœ€è¿‘æˆ‘éƒ½åœ¨å°è¯•ç¼–å†™ä»£ç è·å–æ¨¡å‹çš„åº•å±‚è¾“å‡ºï¼Œåœ¨å››å¤„æŸ¥è¯¢åï¼Œæˆ‘æŸ¥æ‰¾åˆ°äº†è¿™ç¯‡æ–‡ç« ï¼š[Generation](https://huggingface.co/docs/transformers/v4.47.0/zh/main_classes/text_generation#transformers.GenerationConfig.output_scores)ï¼Œé€šè¿‡äº†è§£å¤§æ¨¡å‹è¾“å‡ºæ—¶çš„å„ä¸ªå‚æ•°ï¼Œæˆ‘å‘ç°äº†è¿™ä¸ªï¼š
> [!NOTE]
**output_scores** (bool, optional, defaults to False) â€” Whether or not to return the prediction scores. See scores under returned tensors for more details.
**output_logits** (bool, optional) â€” Whether or not to return the unprocessed prediction logit scores. See logits under returned tensors for more details.

å› æ­¤æˆ‘åœ¨å°è¯•äº†å¤šæ¬¡ä¹‹åä¹Ÿæ˜¯æˆåŠŸç¼–å†™äº†ä¸€ä¸ªå° demoï¼š
```python
import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½æ¨¡å‹å’Œtokenizer
model = AutoModelForCausalLM.from_pretrained('Models/Gemma')
tokenizer = AutoTokenizer.from_pretrained('Models/Gemma')


def generate_text_with_probabilities(model, tokenizer, input_text, max_length=50):
    # ç¼–ç è¾“å…¥æ–‡æœ¬
    input_ids = tokenizer.encode(input_text, return_tensors='pt')

    # å…³é—­æ¢¯åº¦è®¡ç®—ä»¥æé«˜æ•ˆç‡
    with torch.no_grad():
        # ç”Ÿæˆæ–‡æœ¬
        outputs = model.generate(
            input_ids,
            max_length=max_length,
            num_return_sequences=1,
            return_dict_in_generate=True,
            output_scores=True
        )

        # å­˜å‚¨æ¯ä¸ªç”Ÿæˆtokençš„æ¦‚ç‡ä¿¡æ¯
        generated_token_probs = []

        # éå†ç”Ÿæˆçš„tokenåºåˆ—
        for step, logits in enumerate(outputs.scores):
            # ä½¿ç”¨softmaxå°†logitsè½¬æ¢ä¸ºæ¦‚ç‡
            probs = F.softmax(logits, dim=-1)

            # è·å–æ¦‚ç‡æœ€é«˜çš„å‰20ä¸ªtoken
            top_k_probs, top_k_indices = torch.topk(probs[0], k=20)

            # å½’ä¸€åŒ–top-kæ¦‚ç‡
            normalized_top_k_probs = top_k_probs / top_k_probs.sum()

            # å°†tokenåŠå…¶æ¦‚ç‡å­˜å‚¨ä¸ºå­—å…¸
            token_prob_dict = {
                'step': step,
                'tokens': [tokenizer.decode(idx) for idx in top_k_indices],
                'probabilities': normalized_top_k_probs.tolist(),
                'selected_token': tokenizer.decode(outputs.sequences[0][input_ids.shape[1] + step])
            }

            generated_token_probs.append(token_prob_dict)

        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = tokenizer.decode(outputs.sequences[0])

        return {
            'generated_text': generated_text,
            'token_probabilities': generated_token_probs
        }


# æµ‹è¯•å‡½æ•°
input_text = ('The input query is "what is the price of cabbage today?" Please determine whether the query belongs to '
              '"goods" or "life". Just reply "goods" or "life" without any redundant explanations.')
result = generate_text_with_probabilities(model, tokenizer, input_text)

# æ‰“å°ç»“æœ
print("Generated Text:", result['generated_text'])
print("\nToken Probabilities:")
for token_info in result['token_probabilities']:
    print(
        f"\nStep {token_info['step']}, Selected Token: {token_info['selected_token']}, Top 20 Tokens and Probabilities:")
    for token, prob in zip(token_info['tokens'], token_info['probabilities']):
        print(f"{token}: {prob:.4f}")
```
ä½†æ˜¯è¿™ä¸ªæ¨¡å‹æ˜¯æˆ‘ä» huggingface ä¸Šä¸‹è½½ä¸‹æ¥çš„ï¼Œå¹¶ä¸æ˜¯æˆ‘ä»¬éƒ¨é—¨éœ€è¦çš„ï¼Œä½†æ˜¯ç†è®ºèƒ½è¡Œï¼Œæ¥ä¸‹æ¥åªè¦æˆ‘ä»¬èƒ½æˆåŠŸéƒ¨ç½²æˆ‘ä»¬çš„æ¨¡å‹å°±å¯ä»¥è¾¾æˆæˆ‘ä»¬çš„ç›®æ ‡äº†ï¼

## ä»»åŠ¡æäº¤
å½“æˆ‘ä»¬å‰ç½®å·¥ä½œå·²ç»å®Œæˆï¼Œå³æ¨¡å‹å¯ä»¥å®ä¾‹åŒ–ï¼Œsoftmax è¾“å‡ºå¯ä»¥æ‰“å°ï¼Œé‚£ä¹ˆæ¥ä¸‹æ¥æˆ‘ä»¬å°±å°è¯•è§£å†³ç›®å‰é‡åˆ°çš„ä¸€ä¸ªé—®é¢˜ï¼šè§£å†³ float16 æ•°æ®ç±»å‹ï¼Œæ ¹æ®æŸ¥è¯¢ï¼Œè¿™ä¸ªé—®é¢˜åº”è¯¥æ˜¯ç”±äºç‰ˆæœ¬ä¸º CPU çš„é—®é¢˜ï¼Œç”¨ GPU åº”è¯¥å¯ä»¥è§£å†³ï¼Œå› æ­¤æˆ‘ä»¬å°±ä½¿ç”¨ paddlecloud æäº¤ä»»åŠ¡ã€‚
åœ¨ä¹‹å‰çš„æ–‡ç« [PaddleCloudå®¢æˆ·ç«¯å­¦ä¹ ](https://onebuaaer.us.kg/post/PaddleCloud-ke-hu-duan-xue-xi.html)ä¸­å·²ç»ä»‹ç»è¿‡æäº¤æµç¨‹ï¼Œä½†æ˜¯æˆ‘å¿˜è®°äº†ğŸ¤£ï¼Œå› æ­¤æˆ‘åˆèŠ±äº†ä¸€æ®µæ—¶é—´ç†Ÿæ‚‰æ“ä½œï¼Œç„¶åæäº¤äº†ä»»åŠ¡ã€‚
æäº¤ä»»åŠ¡åæˆ‘å°±ä¸€ç›´ç­‰ï¼Œå‘ç°ä»»åŠ¡å§‹ç»ˆå¤„äºæ’é˜ŸçŠ¶æ€ï¼Œå› æ­¤æˆ‘å°±å†³å®šå›å­¦æ ¡ï¼Œä¸‹æ¬¡æ¥å…¬å¸è‚¯å®šå°±èƒ½å‡ºç»“æœäº†ï¼Œå¸Œæœ›æ˜¯å¥½ç»“æœå§ï¼Œæ±‚æ±‚äº†ã€‚

## å…¶ä»–
- ä»Šå¤©å‘çš„æ°´æœæ˜¯ä¸€æ ¹é¦™è•‰ğŸŒã€‚

## æ€»ç»“
ä»Šå¤©çœŸæ˜¯è„‘å­è¦ç‚¸äº†ï¼Œä¸€ç›´åœ¨é…ç¯å¢ƒï¼Œè°ƒå‚æ•°æäº¤ä»»åŠ¡ï¼ŒçœŸæ˜¯å¤ªç…ç†¬äº†ï¼Œæˆ‘ä»€ä¹ˆæ—¶å€™èƒ½æŠŠæ¨¡å‹éƒ¨ç½²å®Œæˆå•ŠğŸ™€ï¼

<!-- ##{"timestamp":1733487962}## -->