ä»Šå¤©æ˜¯å…¥èŒç™¾åº¦çš„ç¬¬2ï¸âƒ£8ï¸âƒ£å¤©ï¼Œè®°å½•ä¸€ä¸‹ä»Šå¤©çš„å†ç¨‹ã€‚

## å¤§è‡´æµç¨‹

```mermaid
graph LR
    A(æ±‡æŠ¥è¿›åº¦)
    B(æ¨¡å‹ä¸‹è½½)
    C(ä»»åŠ¡æäº¤)
    A --> B --> C
```

ä»Šå¤©æƒ³å°½ä¸€åˆ‡åŠæ³•æƒ³æŠŠä»»åŠ¡æäº¤åˆ°é˜Ÿåˆ—ä¸Šå¹¶ä¸”æˆåŠŸè¿è¡Œã€‚

---
æ¥ä¸‹æ¥ä¸€ä¸€ä»‹ç»æ¯ä¸ªéƒ¨åˆ†ï¼š

## è¿›åº¦æ±‡æŠ¥
ä»Šå¤©æ—©ä¸Šè·Ÿmentoræ±‡æŠ¥è¿›åº¦ï¼Œä¸»è¦è¿›å±•å¦‚ä¸‹ï¼š
- å¯¹äºæˆ‘ä»¬ä¹‹å‰è®­ç»ƒçš„æ¨¡å‹ï¼Œæˆ‘å·²ä¸‹è½½åˆ°æœ¬åœ°ï¼Œä½†æ˜¯æ¨¡å‹æ–‡ä»¶è·Ÿå¤§æ¨¡å‹å¸¸è§„ç»“æ„ä¸åŒï¼Œç¼ºå°‘å¾ˆå¤šå…³é”®çš„æ–‡ä»¶ï¼ˆæ¯”å¦‚`config.json`ä¸`model_state.pdparams`ï¼‰ï¼Œè™½ç„¶ç›®å‰å·²ç»è§£å†³ï¼ˆå¤§è‡´è§£å†³æ–¹æ³•ğŸ‘‰[ç¬¬äºŒåå…­å¤©](https://onebuaaer.us.kg/post/bai-du-shi-xi-di-er-shi-liu-tian.html)å’Œ[ç¬¬äºŒåä¸ƒå¤©](https://onebuaaer.us.kg/post/bai-du-shi-xi-di-er-shi-qi-tian.html)ï¼‰ï¼Œä½†æ˜¯å¹¶æ²¡æœ‰å®Œæˆæµ‹è¯•ï¼Œä¸èƒ½ä¿è¯æ¨¡å‹æ–‡ä»¶çš„å®Œæ•´æ€§ä¸å¯ç”¨æ€§ï¼Œå‘¨ä¸‰å·²ç»æäº¤ä»»åŠ¡ï¼Œä½†æ˜¯ä»»åŠ¡æŠ¥é”™äº†ï¼Œä¸»è¦æ˜¯å› ä¸ºæˆ‘é…ç½®çš„å‚æ•°é”™è¯¯ï¼Œæ‰“ç®—å†æ¬¡å°è¯•ï¼š
![image](https://github.com/user-attachments/assets/bb45fc42-f939-46ab-a49a-1038e0c558f9)
- ä¸æ­¤åŒæ—¶æ‰“ç®—ä»huggingfaceä¸Šä¸‹è½½ä¸€ä¸ªChatGLM3-6Bæ¨¡å‹ï¼Œè‡ªå·±è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥ä¿è¯æ¨¡å‹æ–‡ä»¶çš„å®Œæ•´æ€§ä¸å¯ç”¨æ€§
- æ–°è¯æŒ–æ˜ä»»åŠ¡æ–¹é¢ï¼Œè®ºæ–‡å¹¶æ²¡æœ‰æ–°çš„è¿›å±•ğŸ˜…

## å¼€æºæ¨¡å‹æµ‹è¯•
ä¹‹å‰å°±æ³¨å†Œè¿‡huggingfaceè´¦æˆ·ï¼Œå› æ­¤ç›´æ¥ç™»å½•æ‰¾åˆ°æ¨¡å‹å¼€å§‹ä¸‹è½½ï¼Œæ¨¡å‹å¥½å¤§å“¦ï¼Œä¸‹äº†ä¸€ä¸‹åˆï¼š
![image](https://github.com/user-attachments/assets/19307df9-69a2-4e8e-a6a3-529377259a7f)
![image](https://github.com/user-attachments/assets/478ecb94-1e77-42be-8040-344384ff814b)
ç„¶åç®€å•ç¼–å†™äº†ä¸€ä¸ªå°demoæµ‹è¯•æ¨¡å‹çš„å¯ç”¨æ€§ä¸æ˜¯å¦ç¬¦åˆæˆ‘ä»¬çš„éœ€æ±‚ï¼š
```python
import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½æ¨¡å‹å’Œtokenizer
model = AutoModelForCausalLM.from_pretrained('ChatGLM3-6B', trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained('ChatGLM3-6B', trust_remote_code=True)


def generate_text_with_probabilities(model, tokenizer, input_text, max_length=100):
    # ç¼–ç è¾“å…¥æ–‡æœ¬
    input_ids = tokenizer.encode(input_text, return_tensors='pt')

    # å…³é—­æ¢¯åº¦è®¡ç®—ä»¥æé«˜æ•ˆç‡
    with torch.no_grad():
        # ç”Ÿæˆæ–‡æœ¬
        outputs = model.generate(
            input_ids,
            max_length=max_length,
            num_return_sequences=1,
            return_dict_in_generate=True,
            output_scores=True
        )

        # å­˜å‚¨æ¯ä¸ªç”Ÿæˆtokençš„æ¦‚ç‡ä¿¡æ¯
        generated_token_probs = []

        # éå†ç”Ÿæˆçš„tokenåºåˆ—
        for step, logits in enumerate(outputs.scores):
            # ä½¿ç”¨softmaxå°†logitsè½¬æ¢ä¸ºæ¦‚ç‡
            probs = F.softmax(logits, dim=-1)

            # è·å–æ¦‚ç‡æœ€é«˜çš„å‰20ä¸ªtoken
            top_k_probs, top_k_indices = torch.topk(probs[0], k=20)

            # å½’ä¸€åŒ–top-kæ¦‚ç‡
            normalized_top_k_probs = top_k_probs / top_k_probs.sum()

            # å°†tokenåŠå…¶æ¦‚ç‡å­˜å‚¨ä¸ºå­—å…¸
            token_prob_dict = {
                'step': step,
                'tokens': [tokenizer.decode(idx) for idx in top_k_indices],
                'probabilities': normalized_top_k_probs.tolist(),
                'selected_token': tokenizer.decode(outputs.sequences[0][input_ids.shape[1] + step])
            }
            print(token_prob_dict)

            generated_token_probs.append(token_prob_dict)

        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = tokenizer.decode(outputs.sequences[0])

        return {
            'generated_text': generated_text,
            'token_probabilities': generated_token_probs
        }


# æµ‹è¯•å‡½æ•°
input_text = ('The input query is "what is the price of cabbage today?" Please determine whether the query belongs to '
              '"goods" or "life". Just reply "goods" or "life" without any redundant explanations.')
result = generate_text_with_probabilities(model, tokenizer, input_text)

# æ‰“å°ç»“æœ
print("Generated Text:", result['generated_text'])
print("\nToken Probabilities:")
for token_info in result['token_probabilities']:
    print(
        f"\nStep {token_info['step']}, Selected Token: {token_info['selected_token']}, Top 20 Tokens and Probabilities:")
    for token, prob in zip(token_info['tokens'], token_info['probabilities']):
        print(f"{token}: {prob:.4f}")
```
æ¨¡å‹ä¹Ÿæ˜¯æœ‰äº†ç†æƒ³çš„è¾“å‡ºï¼š
```plaintext
Step 5, Selected Token: life, Top 20 Tokens and Probabilities:
life: 0.8956
good: 0.1021
food: 0.0011
life: 0.0006
Life: 0.0002
goods: 0.0001
lif: 0.0001
books: 0.0001
ç”Ÿå‘½: 0.0000
Good: 0.0000
gro: 0.0000
live: 0.0000
good: 0.0000
culture: 0.0000
business: 0.0000
li: 0.0000
loss: 0.0000
family: 0.0000
ç”Ÿæ´»: 0.0000
ç”Ÿå‘½çš„: 0.0000
```
æ¥ä¸‹æ¥æˆ‘ä»¬å°±è¦ç¼–å†™è®­ç»ƒæ¨¡å‹çš„ä»£ç äº†ã€‚
æˆ‘ä»huggingfaceå®˜ç½‘æ‰¾åˆ°æ¨¡å‹å¾®è°ƒç›¸å…³æ–‡ç« [å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹](https://huggingface.co/docs/transformers/zh/training)ï¼Œç„¶åç®€å•ç¼–å†™äº†ä¸€ä¸ªè®­ç»ƒä»£ç ï¼Œåœ¨è‡ªå·±ç”µè„‘ä¸Šå…ˆè¿è¡Œæµ‹è¯•ï¼Œå‰é¢åŠ åœ¨æ¨¡å‹ä¸å¤„ç†è¾“å…¥éƒ½èƒ½æ­£å¸¸è¿è¡Œï¼Œä½†æ˜¯åœ¨è®­ç»ƒé˜¶æ®µå°±å¡ä½äº†ï¼Œè¿™ä¹Ÿæ­£å¸¸ï¼Œç”µè„‘é…ç½®ä¸è¡Œï¼Œäºæ˜¯å°±å‡†å¤‡æäº¤åˆ°é˜Ÿåˆ—ä¸Šè¿è¡Œã€‚

## ä»»åŠ¡æäº¤
ç”±äºä¹‹å‰çš„å‚æ•°è®¾ç½®é”™è¯¯ï¼Œå› æ­¤è¿™æ¬¡æˆ‘æŠŠæ‰€æœ‰éœ€è¦è®¾ç½®çš„å‚æ•°å‰å‰ååæ£€æŸ¥äº†å¤šæ¬¡ï¼Œä¿è¯æ¯æ¡è·¯å¾„çš„å­˜åœ¨ï¼Œç„¶åå°†æ¨¡å‹æ‰“åŒ…ä¸Šä¼ ã€‚ç”±äºæ¨¡å‹å®åœ¨å¤ªå¤§äº†ï¼Œä»»åŠ¡ä¸Šä¼ å°±è¦èŠ±ä¸€ä¸ªå°æ—¶å·¦å³ï¼Œè¿™è¦æ˜¯è¿è¡Œé”™è¯¯ï¼Œä»£ä»·å°±å¤ªå¤§äº†ã€‚
ä»»åŠ¡ä¸Šä¼ ä¹‹åå·²ç»å¾ˆæ™šäº†ï¼Œæˆ‘å°±å›å®¶äº†ï¼Œå¸Œæœ›å‘¨ä¸€æ¥äº†ä¹‹åèƒ½æˆåŠŸã€‚

## å…¶ä»–
- ä»Šå¤©å‘çš„æ°´æœæ˜¯ä¸€ä¸ªè‹¹æœğŸã€‚

## æ€»ç»“
æ¨¡å‹è®­ç»ƒå¥½éº»çƒ¦å•Šï¼Œå•æ˜¯ç¯å¢ƒé…ç½®å°±æŠŠæˆ‘æçš„æ™•å¤´è½¬å‘ã€‚ç»§ç»­åŠ æ²¹ï¼

<!-- ##{"timestamp":1733487962}## -->