该篇文章用于记录训练模型的各种参数与训练效果。

---

# 11月4日 周一

## Task01
- 任务状态：❌
- 参数列表：
<body>
<!--StartFragment-->

GPU卡数 | batch-size | learning-rate | train-epochs | 训练模式 | 模型
-- | -- | -- | -- | -- | --
4 | 8 | 0.00001 | 1 | SFT | FlagAlpha/Llama2-Chinese-13b-Chat

<!--EndFragment-->
</body>

- 失败原因：数据集格式错误，导致数据读取出错。

## Task02
- 任务状态：❌
- 参数列表：
<body>
<!--StartFragment-->

GPU卡数 | batch-size | learning-rate | train-epochs | 训练模式 | 模型
-- | -- | -- | -- | -- | --
4 | 8 | 0.00001 | 1 | SFT | FlagAlpha/Llama2-Chinese-13b-Chat

<!--EndFragment-->
</body>

- 失败原因：GPU耗尽可用内存`MemoryError`。

## Task03
- 任务状态：❌
- 参数列表：
<body>
<!--StartFragment-->

GPU卡数 | batch-size | learning-rate | train-epochs | 训练模式 | 模型
-- | -- | -- | -- | -- | --
4 | 4 | 0.00001 | 1 | SFT | FlagAlpha/Llama2-Chinese-13b-Chat

<!--EndFragment-->
</body>

- 失败原因：未知，日志丢失，无法确定失败原因。

## Task04
- 任务状态：✅
- 参数列表：
<body>
<!--StartFragment-->

GPU卡数 | batch-size | learning-rate | train-epochs | 训练模式 | 模型
-- | -- | -- | -- | -- | --
4 | 4 | 0.00001 | 2 | SFT-LoRA | FlagAlpha/Llama2-Chinese-13b-Chat

<!--EndFragment-->
</body>

- 训练效果：
![image](https://github.com/user-attachments/assets/88a6af35-076a-400c-b21a-3a6f25ada665)
![image](https://github.com/user-attachments/assets/9ce3cdfa-9b9d-4e62-8e97-10a16110d505)

## Task05
- 任务状态：✅
- 参数列表：
<body>
<!--StartFragment-->

GPU卡数 | batch-size | learning-rate | train-epochs | 训练模式 | 模型
-- | -- | -- | -- | -- | --
4 | 4 | 0.00001 | 2 | SFT-LoRA | FlagAlpha/Llama2-Chinese-13b-Chat

<!--EndFragment-->
</body>

>[!NOTE] 
克隆Task04，排除成功的偶然性

## Task06
- 任务状态：❌
- 参数列表：
<body>
<!--StartFragment-->

GPU卡数 | batch-size | learning-rate | train-epochs | 训练模式 | 模型
-- | -- | -- | -- | -- | --
8 | 4 | 0.00001 | 2 | SFT-LoRA | qwen/qwen-14b

<!--EndFragment-->
</body>

- 失败原因：任务一直排队，手动终止。

## Task07
- 任务状态：❌
- 参数列表：
<body>
<!--StartFragment-->

GPU卡数 | batch-size | learning-rate | train-epochs | 训练模式 | 模型
-- | -- | -- | -- | -- | --
4 | 4 | 0.00001 | 2 | SFT-LoRA | qwen/qwen-14b

<!--EndFragment-->
</body>

- 失败原因：任务一直排队，手动终止。

## Task08
- 任务状态：❌
- 参数列表：
<body>
<!--StartFragment-->

GPU卡数 | batch-size | learning-rate | train-epochs | 训练模式 | 模型
-- | -- | -- | -- | -- | --
4 | 4 | 0.00001 | 2 | SFT-LoRA | qwen/qwen-14b

<!--EndFragment-->
</body>

- 失败原因：任务一直排队，手动终止。

## Task09
- 任务状态：❌
- 参数列表：
<body>
<!--StartFragment-->

GPU卡数 | batch-size | learning-rate | train-epochs | 训练模式 | 模型
-- | -- | -- | -- | -- | --
4 | 4 | 0.00001 | 2 | SFT-LoRA | linly-ai/chinese-llama-2-7b

<!--EndFragment-->
</body>

- 失败原因：任务一直排队，手动终止。

## Task10
- 任务状态：❌
- 参数列表：
<body>
<!--StartFragment-->

GPU卡数 | batch-size | learning-rate | train-epochs | 训练模式 | 模型
-- | -- | -- | -- | -- | --
4 | 4 | 0.0001 | 2 | SFT-LoRA | FlagAlpha/Llama2-Chinese-13b-Chat

<!--EndFragment-->
</body>

- 失败原因：克隆任务Task04，可能由于资源队列问题导致运行出错，且日志丢失，无法判定错误原因。

## Task11
- 任务状态：✅
- 参数列表：
<body>
<!--StartFragment-->

GPU卡数 | batch-size | learning-rate | train-epochs | 训练模式 | 模型
-- | -- | -- | -- | -- | --
4 | 4 | 0.00001 | 2 | SFT-LoRA | FlagAlpha/Llama2-Chinese-13b-Chat

<!--EndFragment-->
</body>

- 训练效果：
![image](https://github.com/user-attachments/assets/78bd9db2-5155-4314-a14f-809fb76356b7)
![image](https://github.com/user-attachments/assets/37f6c3c2-b055-4b66-8842-3844d5f6a08a)

> [!CAUTION]
参数与Task04相同但是效果不相同是因为互换了训练与测试数据集。

## Task12
- 任务状态：✅
- 参数列表：
<body>
<!--StartFragment-->

GPU卡数 | batch-size | learning-rate | train-epochs | 训练模式 | 模型
-- | -- | -- | -- | -- | --
4 | 4 | 0.00001 | 2 | SFT-LoRA | qwen/qwen-14b

<!--EndFragment-->
</body>

- 训练效果：
![image](https://github.com/user-attachments/assets/5543d21d-84a0-4141-aec3-731c02565e92)
![image](https://github.com/user-attachments/assets/ae950501-4330-4c33-893f-8cd35a86b879)

- 效果评估：可以看到准确率非常低，猜测可能是由于模型不合适或者学习率设置不恰当。

---

# 11月5日 周二

## Task13
- 任务状态：✅
- 参数列表：
<body>
<!--StartFragment-->

GPU卡数 | batch-size | learning-rate | train-epochs | 训练模式 | 模型
-- | -- | -- | -- | -- | --
4 | 4 | 0.001 | 2 | SFT-LoRA | qwen/qwen-14b

<!--EndFragment-->
</body>

- 训练效果：
![image](https://github.com/user-attachments/assets/748f6051-3dc0-464d-866f-ae1158972227)
![image](https://github.com/user-attachments/assets/902ad591-e56b-4ed5-91c7-ba952e54d7d7)

- 效果评估：在修改学习率后，相比Task12有了显著提升，接下来还可以继续尝试其他学习率尝试改进。

## Task14
- 任务状态：❎
- 参数列表：
<body>
<!--StartFragment-->

GPU卡数 | batch-size | learning-rate | train-epochs | 训练模式 | 模型
-- | -- | -- | -- | -- | --
4 | 4 | 0.01 | 2 | SFT-LoRA | qwen/qwen-14b

<!--EndFragment-->
</body>

- 失败原因：log日志文件丢失，失败原因未知，猜测可能是资源队列问题，决定重跑作业，标记为Task16。

## Task15
- 任务状态：✅
- 参数列表：
<body>
<!--StartFragment-->

GPU卡数 | batch-size | learning-rate | train-epochs | 训练模式 | 模型
-- | -- | -- | -- | -- | --
4 | 4 | 0.01 | 3 | SFT-LoRA | qwen/qwen-14b

<!--EndFragment-->
</body>

- 训练效果：
![image](https://github.com/user-attachments/assets/643afe40-cf52-44e6-8add-7499e2a65002)
![image](https://github.com/user-attachments/assets/015509b9-ac74-4635-a9e5-dc5d1aa02398)

- 效果评估：正确率相比Task13略有提升，但无法确定是由于learning-rate改变还是train-epochs的调整导致，需要查看Task16训练结果。

## Task16
- 任务状态：❎
- 参数列表：
<body>
<!--StartFragment-->

GPU卡数 | batch-size | learning-rate | train-epochs | 训练模式 | 模型
-- | -- | -- | -- | -- | --
4 | 4 | 0.01 | 2 | SFT-LoRA | qwen/qwen-14b

<!--EndFragment-->
</body>

- 