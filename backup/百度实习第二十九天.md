ä»Šå¤©æ˜¯å…¥èŒç™¾åº¦çš„ç¬¬2ï¸âƒ£9ï¸âƒ£å¤©ï¼Œè®°å½•ä¸€ä¸‹ä»Šå¤©çš„å†ç¨‹ã€‚

## å¤§è‡´æµç¨‹

```mermaid
graph LR
    A(ä»»åŠ¡æäº¤)
    A
```

ä»Šå¤©æƒ³å°½ä¸€åˆ‡åŠæ³•è®­ç»ƒæ¨¡å‹ã€‚

---
æ¥ä¸‹æ¥ä¸€ä¸€ä»‹ç»æ¯ä¸ªéƒ¨åˆ†ï¼š

## ä»£ç ç¼–å†™
å…¶å®æ²¡å•¥å¥½è¯´çš„ï¼Œå°±æ˜¯å„ç§é—®é¢˜ï¼Œå„ç§æŠ¥é”™ï¼ŒåŸå› åº”æœ‰å°½æœ‰ï¼Œåªæœ‰ä½ æƒ³ä¸åˆ°ï¼Œæ²¡æœ‰ä»–åšä¸åˆ°ã€‚
æ”¹æ”¹æ”¹â€¦â€¦æ”¹æ”¹æ”¹â€¦â€¦æ”¹æ”¹æ”¹â€¦â€¦æ”¹æ”¹æ”¹â€¦â€¦æ”¹æ”¹æ”¹â€¦â€¦æ”¹æ”¹æ”¹â€¦â€¦æ”¹æ”¹æ”¹â€¦â€¦æ”¹æ”¹æ”¹â€¦â€¦æ”¹æ”¹æ”¹â€¦â€¦æ”¹æ”¹æ”¹â€¦â€¦æ”¹æ”¹æ”¹â€¦â€¦æ”¹æ”¹æ”¹â€¦â€¦
æ”¹äº†åå¤šæ¬¡ï¼ŒåŒ…æ‹¬åé¢è¦æåˆ°çš„Google Colabã€‚
è€å¤©çˆ·å‘€ï¼Œå¯æ€œå¯æ€œæˆ‘å§ï¼

## ä»»åŠ¡æäº¤
å…ˆçœ‹ä¸€ä¸‹æˆ‘çš„æˆ˜æœå§ï¼š
![image](https://github.com/user-attachments/assets/b39ddce0-d8b9-437e-91d1-3a217a4e3320)
å“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå•Šå•Šå•Šå•Šå•Šå•Šå•Šå‘œå‘œå‘œå‘œå‘œå‘œå‘œâ€¦â€¦
è™½ç„¶ä»»åŠ¡æäº¤æˆ‘ä¸éœ€è¦ä¸€ç›´ç­‰ç€ï¼Œä½†æ˜¯è¿™ä¸€ä¸ªå°æ—¶çš„ä»£ç è®©æˆ‘æ¯æ¬¡çœ‹åˆ°å¤±è´¥éƒ½å¾ˆéš¾å—å•Šï¼
æˆ‘åˆ°åº•ä»€ä¹ˆæ—¶å€™æ‰èƒ½æˆåŠŸå•Šï¼
æˆ‘å†ä¸åšå‡ºæ¥ç‚¹ä¸œè¥¿è·Ÿmentoréƒ½æ²¡ä»€ä¹ˆæ±‡æŠ¥çš„äº†ï¼Œå¥½ç„¦è™‘å•Šï¼

## åˆ‡æ¢å¹³å°
å¾€é˜Ÿåˆ—ä¸Šæäº¤ä»»åŠ¡å¤ªéš¾äº†ï¼Œæäº¤äº†å¥½å¤šäº†ï¼Œä¸€ç›´å¤±è´¥ä¸€ç›´å¤±è´¥ï¼Œä¸æ˜¯ç¯å¢ƒé—®é¢˜ï¼Œå°±æ˜¯pythonåŒ…ç‰ˆæœ¬é—®é¢˜ï¼Œä¸æ˜¯ä»£ç é—®é¢˜ï¼Œå°±æ˜¯æ•°æ®å¤„ç†é—®é¢˜ã€‚åœ¨æœ¬åœ°è§£å†³æåˆ°çš„é—®é¢˜åè¿˜è¦èŠ±ä¸€ä¸ªå°æ—¶æäº¤ä»»åŠ¡ï¼Œå› æ­¤æˆ‘å°±åœ¨æƒ³ï¼Œæœ‰æ²¡æœ‰æ›´æ–¹ä¾¿çš„æ–¹æ³•å‘¢ï¼ŸğŸ’¡æƒ³åˆ°äº†ï¼šGoogle Colabã€‚
äºæ˜¯æˆ‘ç™»å½•Google Colabï¼Œç„¶åå°†è‡ªå·±çš„æ•°æ®é›†ä¸Šä¼ åˆ°huggingfaceï¼Œè¿™æ ·å°±å¯ä»¥ç›´æ¥é€šè¿‡huggingfaceè®¿é—®äº†ã€‚
â€¦â€¦
â€¦â€¦
ä¸­é—´è¿‡ç¨‹å°±ç®€å•è¯´ä¸€ä¸‹å§ï¼Œä¹Ÿæ˜¯é‡åˆ°äº†å¾ˆå¤šé—®é¢˜ï¼Œæ¯”å¦‚æ•°æ®å¤„ç†å•Šï¼Œpythonåº“é€‰æ‹©å•Šï¼Œlossè®¡ç®—å•Šç­‰ç­‰ç­‰ç­‰â€¦â€¦æœ€ç»ˆä¹Ÿæ˜¯æ•²å®šäº†ä¸€ç‰ˆä»£ç ï¼š
```python
!pip install datasets evaluate transformers

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import torch
import evaluate

# åŠ è½½æ•°æ®é›†
dataset = load_dataset("wlkla/query-classification")

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)

# åˆ›å»ºæ ‡ç­¾æ˜ å°„
unique_labels = list(set(dataset['train']['Label']))
label2id = {label: idx for idx, label in enumerate(unique_labels)}
id2label = {idx: label for label, idx in label2id.items()}

# åˆ†è¯å‡½æ•°
def tokenize_function(examples):
    return tokenizer(
        examples["Query"],
        padding="max_length",
        truncation=True,
        max_length=64
    )

# è½¬æ¢æ ‡ç­¾å¹¶åˆ†è¯
def preprocess_labels(example):
    example["labels"] = label2id[example["Label"]]  # è½¬æ¢æ ‡ç­¾ä¸ºæ•°å­—
    return example

tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=["Query"])
tokenized_datasets = tokenized_datasets.map(preprocess_labels)

# è®¾ç½®æ•°æ®æ ¼å¼ä¸º PyTorch
tokenized_datasets.set_format("torch")

# åŠ è½½æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained(
    "THUDM/chatglm3-6b",
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id,
    trust_remote_code=True
)

# åŠ è½½è¯„ä¼°æŒ‡æ ‡
metric = evaluate.load("accuracy")

# å®šä¹‰è¯„ä¼°å‡½æ•°
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = torch.argmax(torch.tensor(logits), dim=-1).numpy()
    return metric.compute(predictions=predictions, references=labels)

# å®šä¹‰è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./query_classification_results",
    num_train_epochs=3,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="epoch",
    fp16=True
)

# å®šä¹‰ Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'],
    compute_metrics=compute_metrics,
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```
ä½†æ˜¯è¿è¡ŒæŠ¥é”™å•¦ğŸ˜­ï¼š
![image](https://github.com/user-attachments/assets/e65d4751-c532-46a4-b9e1-551c8e2515a5)
æˆ‘ç°åœ¨å·²ç»ä¸æƒ³æŠ±æ€¨äº†ï¼Œæäº†ä¸€ä¸‹åˆï¼Œè¿˜æ˜¯ä¸€ç›´å¤±è´¥ï¼Œæˆ‘ç°åœ¨å†…å¿ƒæ¯«æ— æ³¢æ¾œï¼Œå·²ç»å¤±æœ›äº†ğŸ˜ã€‚

## å…¶ä»–
- å°ç»“ä¸€ä¸‹ï¼Œä»Šå¤©æ”¹äº†ä¸€å¤©ä»£ç ï¼Œæäº†ä¸€å¤©ä»»åŠ¡ï¼Œå…¨å¤±è´¥äº†ï¼Œæˆ‘ç°åœ¨å¾ˆç—›è‹¦ğŸ’”ï¼
- ä»Šå¤©å‘çš„æ°´æœæ˜¯ä¸€ä¸ªæ©˜å­ğŸŠã€‚

## æ€»ç»“
å•Šå•Šå•Šå•Šå•Šï¼Œç—›è‹¦å•Šï¼Œæˆ‘åªæ˜¯æƒ³è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œä¸ºä»€ä¹ˆé‚£ä¹ˆéš¾ï¼Œä»»åŠ¡æäº¤å¥½éº»çƒ¦å•Šï¼Œç»™æˆ‘ä¸€å°é«˜é…ç”µè„‘å¤šå¥½å•Šï¼Œæˆ‘å°±ä¸ç”¨è€ƒè™‘åˆ°åº•æ€ä¹ˆæäº¤ä»»åŠ¡äº†ï¼Œæ€ä¹ˆåœ¨è¿œç¨‹é…ç½®ç¯å¢ƒäº†ï¼Œåˆ†æ˜æ€è·¯å¾ˆæ˜ç¡®çš„äº‹ï¼Œå´ç”±äºç¯å¢ƒä¸€ç›´ä¸æˆåŠŸï¼Œç—›ï¼Œå¤ªç—›å•¦ï¼ï¼

<!-- ##{"timestamp":1733747162}## -->