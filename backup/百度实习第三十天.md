今天是入职百度的第3️⃣0️⃣天，记录一下今天的历程。

## 大致流程

```mermaid
graph LR
    A(任务提交)
    B(论文浮现)
    A --> B
```

今天在任务提交之余尝试复现论文。

---
接下来一一介绍每个部分：

## 任务提交
今天又提交了五六个任务（提交效率太低了，一个就要一个半小时左右），然后不出所料，全部出错了，类型各种各样，大致包含：
- paddlecloud和paddlnlp版本错误
- paddlenlp与transformers不可搭配使用错误
- python库导入错误
- python库版本错误
- 平台python版本问题
- ……

我按照报错内容一遍遍修改代码，解决了前两个问题。后面遇到的问题是针对transformers安装问题，直接`pip install transformers`总是安装不成功，因此下载transformers库，然后上传队列，手动安装，问题就解决了。
虽然能成功安装，但是会警告当前python版本过低。直接'conda create -n model_train python=3.10'的话会报错，说识别不了conda，然后又找了好久的文章，在本地创建环境然后打包上传队列，解压后启动。
解决这个问题之后又报错说transformers与torch不匹配，需要安装`transformers[torch]`或者`accelerate`。因此又在脚本中添加对应语句。
虽然上面只显示了三个问题，其实在解决过程中特别曲折，重复提交了五六次才解决上面的问题。安装完`transformers[torch]`后已经很晚了，我就回学校了。

## 论文复现
先看一下我的战果吧：
![image](https://github.com/user-attachments/assets/d60e4959-d26e-43ca-b222-92882137334b)
根据论文内容大致规划了一下思路，然后就开始尝试复现。
复现过程中遇到以下问题：**分词的准确性本身就依赖于词库的完整性，如果词库中根本没有新词，则信任分词结果无法被信任**
因此根据自己以往的项目经历以及网上的一些文章，我尝试了以下思路：**不依赖词库进行分词，而是自己制定一种方法对文本进行分词，然后将结果与已有词库进行对比，则可筛选出候选词**
因此我们将输入文本全部拆分成$2\leqslant len(token) \leqslant length$的文本片段（length为指定长度），然后对所有的片段计算其自由度与内部聚合度，筛选出我们需要的新词。
目前也是实现了一小部分，拆分和存储。这里我们使用Trie树进行存储，可以更高效的查询与获取。

## 其他
- 今天提了一天任务，全失败了；写了半天代码，还没写完，😭！
- 今天发的水果是一个香蕉🍌。

## 总结
好痛苦啊，不要总结了！

<!-- ##{"timestamp":1733831355}## -->